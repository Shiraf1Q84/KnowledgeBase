import os
import streamlit as st
import openai
from llama_index.llms.openai import OpenAI
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core import Settings
# from llama_index.indices.list import ListIndex

import tempfile
import os
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
)
from llama_index.embeddings.openai import OpenAIEmbedding

try:
    from llama_index import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader
except ImportError:
    from llama_index.core import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader,ListIndex

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="",
    page_icon="ğŸ¦™",
    layout="centered",
    initial_sidebar_state="auto",
    menu_items=None
)

import os
from dotenv import load_dotenv

load_dotenv() # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€

api_key = os.getenv('API_KEY') # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã™ã‚‹


# Temperature slider
temperature = st.slider(
    "Temperature (Creativity)", min_value=0.0, max_value=2.0, value=0.8, step=0.1
)

if api_key:
    st.session_state.api_key = api_key
    openai.api_key = api_key

import streamlit as st
import openai

# APIã‚­ãƒ¼ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®å¤‰æ•°
if "api_key" not in st.session_state:
    st.session_state.api_key = ""

# APIã‚­ãƒ¼å…¥åŠ›æ¬„
st.title("OpenAI API ã‚­ãƒ¼å…¥åŠ›")
st.text_input("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", key="api_key", value=st.session_state.api_key)

# å…¥åŠ›ã•ã‚ŒãŸAPIã‚­ãƒ¼ã‚’openaiã«è¨­å®š
if st.button("APIã‚­ãƒ¼ã‚’è¨­å®š"):
    openai.api_key = st.session_state.api_key
    st.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ")


# PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
pdf_folder = r"C:\Users\MI1935\Downloads\â– â– â– 2024-03-21_PROJECT_FILE\RAG\data"


# --- ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ (äº‹å‰æ§‹ç¯‰) ---
@st.cache_resource(show_spinner=False)
def build_vector_database():
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¹ãƒ”ãƒŠãƒ¼ã‚’è¡¨ç¤º
    with st.spinner(text="Wait minites."):

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        for document in docs:
            document.metadata["filename"] = document.source  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
        embed_model = OpenAIEmbedding(openai_api_key=openai.api_key)

        # Create a vector store index from the documents
        index = VectorStoreIndex.from_documents(docs)
        return index

# ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰
index = build_vector_database()

# --- ãƒãƒ£ãƒƒãƒˆUI ---

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼"}
    ]

# LLMã¨query engineã®è¨­å®š (APIã‚­ãƒ¼ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿)
if openai.api_key:
    llm = OpenAI(
        model="gpt-4o",
        temperature=temperature,
    )

# --- ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ– (å¤‰æ›´ç‚¹) ---
if "chat_engine" not in st.session_state.keys() and openai.api_key is not None:
    similarity_top_k = 5  # é¡ä¼¼ã™ã‚‹ã‚‚ã®ã‚’5ã¤æ¤œç´¢
    postprocessor = SimilarityPostprocessor(similarity_top_k=similarity_top_k)
    system_prompt ="""
    ãƒ»ã‚ãªãŸã¯ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«æä¾›ã•ã‚Œã¦ã„ã‚‹æ›¸é¡ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æä¾›ã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
    ãƒ»åˆ©ç”¨è€…ã®è³ªå•ã«ã€æ­£ç¢ºã‹ã¤ãªã‚‹ã¹ãè©³ç´°ã«ã€å‚è€ƒè³‡æ–™ã‚’å¼•ç”¨ã—ãªãŒã‚‰ç­”ãˆã‚‹ã“ã¨ãŒã‚ãªãŸã®å½¹å‰²ã§ã™ã€‚
    ãƒ»æƒ…å ±ã¯800æ–‡å­—ä»¥ä¸Šã€4000æ–‡å­—ä»¥å†…ã«åã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
    ãƒ»è©³ç´°ãªæƒ…å ±ãŒå¿…è¦ãªå ´åˆã¯ã€åˆ©ç”¨è€…ã‹ã‚‰è¿½åŠ ã®è³ªå•ã‚’ä¿ƒã—ã¾ã™ã€‚
    ãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è¦‹ã‚„ã™ãå‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    ãƒ»æƒ…å ±æºã‚’æ˜è¨˜ã—ã¦å›ç­”ã™ã‚‹ã‚ˆã†ã«åŠªã‚ã¾ã™ã€‚
    ãƒ»è¤‡æ•°ã®è§£é‡ˆãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã‚’æç¤ºã—ã¾ã™ã€‚
    ãƒ»ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã ã‘ã§ã¯åˆ¤æ–­ã§ããªã„å ´åˆã«ã¯ã€åˆ¤æ–­ã§ããªã„æ—¨ã‚’ä¼ãˆã¾ã™ã€‚
    ãƒ»åˆ¤æ–­ã«ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ãŒã‚ã‚Œã°ã€è¿½åŠ ã§æƒ…å ±ã‚’æ±‚ã‚ã¾ã™ã€‚
    ãƒ»å¿…è¦ã«å¿œã˜ã¦ã€é–¢é€£ã™ã‚‹æƒ…å ±æºã¸ã®ãƒªãƒ³ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
    ãƒ»ä¸­ç«‹çš„ãªç«‹å ´ã‚’ä¿ã¡ã€åã£ãŸæƒ…å ±æä¾›ã¯è¡Œã„ã¾ã›ã‚“ã€‚
    """  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
    st.session_state.chat_engine = index.as_chat_engine(
        chat_mode="context",
        verbose=True,
        postprocessor=postprocessor,
        llm=llm,  # llmå¤‰æ•°ã‚‚å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™
        system_prompt=system_prompt  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
    )


# ãƒãƒ£ãƒƒãƒˆå‡¦ç†
if openai.api_key  is not None:
    if prompt := st.chat_input("Your question"):
        st.session_state.messages.append({"role": "user", "content": prompt})

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = st.session_state.chat_engine.chat(prompt)
                st.write(response.response)
                message = {"role": "assistant", "content": response.response}
                st.session_state.messages.append(message)


                # for source in response.source_nodes:
                #     st.write(f"**å‚è€ƒç®‡æ‰€:**")
                #     st.markdown(source.node.get_content())  # Display node content directly

                for source in response.source_nodes:
                    st.write(f"**å‚è€ƒç®‡æ‰€:** {source.node.metadata['filename']}") # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã“ã“ã«è¡¨ç¤º
                    st.markdown(source.node.get_content())  # Display node content directly
