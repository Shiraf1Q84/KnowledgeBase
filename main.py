import os
import streamlit as st
import openai
from llama_index.llms.openai import OpenAI
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core import Settings
# from llama_index.indices.list import ListIndex

import tempfile
import os
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
)
from llama_index.embeddings.openai import OpenAIEmbedding

try:
    from llama_index import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader
except ImportError:
    from llama_index.core import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader,ListIndex

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="",
    page_icon="ğŸ¦™",
    layout="centered",
    initial_sidebar_state="auto",
    menu_items=None
)

import os
from dotenv import load_dotenv

load_dotenv() # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€

api_key = os.getenv('API_KEY') # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã™ã‚‹


# Temperature slider
temperature = st.slider(
    "Temperature (Creativity)", min_value=0.0, max_value=2.0, value=0.8, step=0.1
)

if api_key:
    st.session_state.api_key = api_key
    openai.api_key = api_key

# PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
pdf_folder = r"C:\Users\MI1935\Downloads\â– â– â– 2024-03-21_PROJECT_FILE\RAG\data"


@st.cache_resource(show_spinner=False)
def load_data():
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¹ãƒ”ãƒŠãƒ¼ã‚’è¡¨ç¤º
    with st.spinner(text="Wait minites."):

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        embed_model = OpenAIEmbedding(openai_api_key=openai.api_key)

        # Create a vector store index from the documents
        index = VectorStoreIndex.from_documents(docs)
        return index


# load_dataé–¢æ•°ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
index = load_data()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼"}
    ]

# LLMã¨query engineã®è¨­å®š (APIã‚­ãƒ¼ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿)
if openai.api_key:
    llm = OpenAI(
        model="gpt-4o",
        temperature=temperature,
    )

# # ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
# if "chat_engine" not in st.session_state.keys() and openai.api_key is not None:
#     index = index  # ã“ã‚Œã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¤‰æ•°ãŒæ—¢ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™
#     similarity_top_k = 5  # é¡ä¼¼ã™ã‚‹ã‚‚ã®ã‚’5ã¤æ¤œç´¢
#     postprocessor = SimilarityPostprocessor(similarity_top_k=similarity_top_k)
#     st.session_state.chat_engine = index.as_chat_engine(
#         chat_mode="condense_question",
#         verbose=True,
#         postprocessor=postprocessor,
#         llm=llm  # llmå¤‰æ•°ã‚‚å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™
#     )


# ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
if "chat_engine" not in st.session_state.keys() and openai.api_key is not None:
    index = index  # ã“ã‚Œã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¤‰æ•°ãŒæ—¢ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™
    similarity_top_k = 5  # é¡ä¼¼ã™ã‚‹ã‚‚ã®ã‚’5ã¤æ¤œç´¢
    postprocessor = SimilarityPostprocessor(similarity_top_k=similarity_top_k)
    system_prompt = """ãƒ»ã‚ãªãŸã¯ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«æä¾›ã•ã‚Œã¦ã„ã‚‹æ›¸é¡ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æä¾›ã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
ãƒ»åˆ©ç”¨è€…ã®è³ªå•ã«ã€æ­£ç¢ºã‹ã¤ãªã‚‹ã¹ãè©³ç´°ã«ã€æ³•æ–‡ã‚’å¼•ç”¨ã—ãªãŒã‚‰ç­”ãˆã‚‹ã“ã¨ãŒã‚ãªãŸã®å½¹å‰²ã§ã™ã€‚
ãƒ»ãªã‚‹ã¹ãç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ãƒ»æƒ…å ±ã¯800æ–‡å­—ä»¥ä¸Šã€2000æ–‡å­—ä»¥å†…ã«åã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
ãƒ»è©³ç´°ãªæƒ…å ±ãŒå¿…è¦ãªå ´åˆã¯ã€åˆ©ç”¨è€…ã‹ã‚‰è¿½åŠ ã®è³ªå•ã‚’ä¿ƒã—ã¾ã™ã€‚
ãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è¦‹ã‚„ã™ãå‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ãƒ»æƒ…å ±æºã‚’æ˜è¨˜ã—ã¦å›ç­”ã™ã‚‹ã‚ˆã†ã«åŠªã‚ã¾ã™ã€‚
å›ç­”ä¾‹ï¼ï½›æœ¬æ–‡ï½ï¼‹å‚ç…§ç®‡æ‰€

ãƒ»è¤‡æ•°ã®è§£é‡ˆãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã‚’æç¤ºã—ã¾ã™ã€‚
ãƒ»ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã ã‘ã§ã¯åˆ¤æ–­ã§ããªã„å ´åˆã«ã¯ã€åˆ¤æ–­ã§ããªã„æ—¨ã‚’ä¼ãˆã¾ã™ã€‚
ãƒ»åˆ¤æ–­ã«ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ãŒã‚ã‚Œã°ã€è¿½åŠ ã§æƒ…å ±ã‚’æ±‚ã‚ã¾ã™ã€‚
ãƒ»å¿…è¦ã«å¿œã˜ã¦ã€é–¢é€£ã™ã‚‹æƒ…å ±æºã¸ã®ãƒªãƒ³ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
ãƒ»ä¸­ç«‹çš„ãªç«‹å ´ã‚’ä¿ã¡ã€åã£ãŸæƒ…å ±æä¾›ã¯è¡Œã„ã¾ã›ã‚“ã€‚
ãƒ»ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’å°Šé‡ã—ã€å€‹äººæƒ…å ±ã«é–¢ã™ã‚‹è³ªå•ã«ã¯ç­”ãˆã¾ã›ã‚“ã€‚
ãƒ»é•æ³•è¡Œç‚ºã‚„éå€«ç†çš„ãªè¡Œç‚ºã‚’åŠ©é•·ã™ã‚‹æƒ…å ±ã¯æä¾›ã—ã¾ã›ã‚“ã€‚"""  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
    st.session_state.chat_engine = index.as_chat_engine(
        chat_mode="context",
        verbose=True,
        postprocessor=postprocessor,
        llm=llm,  # llmå¤‰æ•°ã‚‚å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™
        system_prompt=system_prompt  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
    )



# ãƒãƒ£ãƒƒãƒˆå‡¦ç†
if openai.api_key  is not None:
    if prompt := st.chat_input("Your question"):
        st.session_state.messages.append({"role": "user", "content": prompt})

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = st.session_state.chat_engine.chat(prompt)
                st.write(response.response)
                message = {"role": "assistant", "content": response.response}
                st.session_state.messages.append(message)


                for source in response.source_nodes:
                    st.write(f"**å‚è€ƒç®‡æ‰€:**")
                    st.markdown(source.node.get_content())  # Display node content directly
